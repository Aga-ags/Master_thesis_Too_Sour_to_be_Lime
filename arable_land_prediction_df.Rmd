---
title: "Preparing for the predictions for entire area"
author: "Agnieszka Kubica"
date: "`r Sys.Date()`"
output: html_document
---

```{r libraries}
library(tidyverse)
library(terra)
library(sf)
library(ggmice)
#library(obliquer)
library(recipes)
library(FNN)
```

# Making of the dataframe for prediction based on the agricultural area raster 20x20 
```{r}
# Create a list of paths to all of the tifs 
numeric_folder <- "../data/covariates/numeric"
tif_files_numeric <- list.files(numeric_folder, pattern = "\\.tif$", full.names = TRUE)

categoric_folder <- "../data/covariates/categoric"
tif_files_categoric <- list.files(categoric_folder, pattern = "\\.tif$", full.names = TRUE)

# Load as rasters
rasters_num_list <- lapply(tif_files_numeric, rast)
rasters_cat_list <- lapply(tif_files_categoric, rast)

# Assign EPSG:2056 (the CRS are rasters should already be in)
rasters_num_list <- lapply(rasters_num_list, function(r) {
  crs(r) <- "EPSG:2056"
  return(r)
})
rasters_cat_list <- lapply(rasters_cat_list, function(r) {
  crs(r) <- "EPSG:2056"
  return(r)
})

# Pick agricultural raster as reference
areable_land_raster <- terra::rast("../data/zh_arable_land_s3.tif")
crs(areable_land_raster)  <- "epsg:2056"
ref_raster <- areable_land_raster

resample_dynamic <- function(r, ref_raster) {
  res_original <- res(r)
  avg_res <- mean(res_original)  # or use max/min depending on your rule

  # Define threshold logic
  if (avg_res <= 5) {
    method <- "average"
  } else {
    method <- "bilinear"
  }

  r_resampled <- resample(r, ref_raster, method = method)
  
  # Save to disk to avoid memory overflow
  out_path <- file.path(paste0("../data/resampled_tiffs/", names(r), "_resampled.tif"))
  writeRaster(r_resampled, out_path, overwrite = TRUE)

  # Load back from disk (lightweight in memory)
  rast(out_path)
  return(r_resampled)
}

# Apply to numeric rasters
rasters_aligned_num <- lapply(rasters_num_list, resample_dynamic, ref_raster)

rasters_aligned_cat <- lapply(rasters_cat_list, function(r) {
  # Resample to match resolution and extent
  r_resampled <- resample(r, ref_raster, method = "near")
  return(r_resampled)
})

# Combine both numeric and categorical rasters
all_rasters <- c(rasters_aligned_num, rasters_aligned_cat)

# Stack into a single SpatRaster
stacked_rasters <- rast(all_rasters)

# Extract base names without extension
layer_names <- c(
  tools::file_path_sans_ext(basename(tif_files_numeric)),
  tools::file_path_sans_ext(basename(tif_files_categoric))
)

names(stacked_rasters) <- layer_names

writeRaster(stacked_rasters, "../data/resampled_tiffs/stacked_raster.tif")
```


```{r arable_land}
# load things in again
stacked_rasters <- rast("../data/resampled_tiffs/stacked_raster.tif")
areable_land_raster <- terra::rast("../data/zh_arable_land_s3.tif")
crs(areable_land_raster)  <- "epsg:2056"

# Filter only cells in arable land
# Mask the full raster stack where there is no arable_land 
areable_land_raster <- classify(areable_land_raster, cbind(NA, 0)) # convert NA to 0
masked_rasters <- mask(stacked_rasters, areable_land_raster, maskvalue = 0, updatevalue = -999) # by default updatevalue is set to NA, but then we cannot distinguish true NA from non arable land

# Convert into a dataframe
df_arable <- as.data.frame(masked_rasters, xy = TRUE, na.rm = FALSE)|>
  filter(asp_eness100m != -999) # filter out non arable land

nrow(df_arable)
```

```{r}
 write.csv(df_arable, file = '../data/prepared_data/map_data_ref_arable_no_preprocessing.csv', row.names=FALSE)
```


# Prepare the soil covariates

```{r}
df_arable <- read.csv(file='../data/prepared_data/map_data_ref_arable_no_preprocessing.csv') # 20 by 20
df_arable
```

## Missing data
```{r check_missing}
missing_df <- df_arable[!complete.cases(df_arable), ]%>%
  dplyr::select(where(~ anyNA(.)) | c(x, y))
# There is a lot of missing data

plot_pattern(missing_df, rotate = TRUE)
```

```{r missing_data}
# Impute the missing data in variables that needed it before 

df_arable <- df_arable|>
  mutate(skfeucht = ifelse(is.na(skfeucht), 0, skfeucht),
         gwleiter = ifelse(is.na(gwleiter), 0, gwleiter),
         feucht_wild =  ifelse(is.na(feucht_wild), 0, feucht_wild), 
          bedgwleiter  = ifelse(is.na(bedgwleiter), 0, bedgwleiter))
```


```{r}
# impute drainfor
points_missing_drainfor <- missing_df |>
  filter(is.na(drainfor))|>
  dplyr::select(x,y,drainfor)|>
    st_as_sf(coords = c("x", "y"), crs = 2056)

#raster file
r_drainfor <- terra::rast("../data/covariates/numeric/drainfor.tif")
crs(r_drainfor) <- "EPSG:2056"

# Get all non-NA raster cell indices and their values
r_vals <- values(r_drainfor)
non_na_cells <- which(!is.na(r_vals))
non_na_coords <- terra::xyFromCell(r_drainfor, non_na_cells)
non_na_values <- r_vals[non_na_cells]

# Get coordinates of missing points (already an sf object)
missing_coords <- st_coordinates(points_missing_drainfor)

# Find nearest non-NA raster cell for each missing point
nn <- get.knnx(non_na_coords, missing_coords, k = 1)

# Impute values
points_missing_drainfor$drainfor <- non_na_values[nn$nn.index[,1]]

#Drop geometry
imputed_df <- points_missing_drainfor %>%
  mutate(x = st_coordinates(.)[,1],
         y = st_coordinates(.)[,2]) %>%
  st_drop_geometry() %>%
  select(x, y, drainfor)  # only keep relevant columns

# Join imputed values back into original dataframe
df_arable_imputed <- df_arable %>%
  left_join(imputed_df, by = c("x", "y"), suffix = c("", "_imputed")) %>%
  mutate(drainfor = ifelse(is.na(drainfor), drainfor_imputed, drainfor)) %>%
  select(-drainfor_imputed)
```


```{r}

# Checking what might be happening with remaining missing data:
missing_df_spatial <- missing_df|>
  st_as_sf(coords = c("x", "y"), crs = 2056)  

ggplot() +
  geom_sf(data = missing_df_spatial, aes(color = is.na(morriss), alpha = 0.3)) 

ggplot() +
  geom_sf(data = missing_df_spatial, aes(color = is.na(zh_gwn25_vdist), alpha = 0.3)) 


ggplot() +
  geom_sf(data = missing_df_spatial, aes(color = is.na(te6190avgty), alpha = 0.3)) 

ggplot() +
  geom_sf(data = missing_df_spatial, aes(color = is.na(geoform_fil), alpha = 0.3)) 

ggplot() +
  geom_sf(data = missing_df_spatial, aes(color = is.na(caco3), alpha = 0.3)) 
# seems quite randomly missing

ggplot() +
  geom_sf(data = missing_df_spatial, aes(color = is.na(drain), alpha = 0.3)) 
# the raster is smaller than arable land

```

```{r extract_complete_cases}
# Decision: remove the remaining missing data
 df_arable_complete <- df_arable[complete.cases(df_arable), ]
```

```{r}
 write.csv(df_arable_complete, file = '../data/prepared_data/map_data_ref_arable_complete.csv', row.names=FALSE)
```

## Convert variable type

```{r}
df_arable_complete <- read.csv(file='../data/prepared_data/map_data_ref_arable_complete.csv') # 20 by 20
df_arable_complete
```

```{r}
df_arable_processed <- df_arable_complete |>
  mutate(across(75:90, as.factor))|> # CHECK
  mutate(bedgwleiter = as.numeric(bedgwleiter), # these were incorrectly processed in the data preparing code before
         feucht_wild = as.numeric(feucht_wild),
         gwleiter = as.numeric(gwleiter),
         skfeucht= as.numeric(skfeucht))

df_arable_processed
```

## Add remaining covariates 
```{r OBCs}
# creating the oblique geographic coordinates based on a raster

my_cov <- terra::rast("../data/covariates/numeric/drumdist.tif")
crs(my_cov) <- "EPSG:2056"
ogcs <- obliquify(my_cov, 10) #drastic increase in prediction performance stopped around here for most models
ogcs

# add the OBCs to dataframe
points_for_rotation <- df_arable_processed|>
  vect(geom = c("x", "y"), crs = "EPSG:2056")

df_arable_with_OBCs <- extract(ogcs, points_for_rotation, bind = TRUE)
df_arable_with_OBCs_values <- values(df_arable_with_OBCs)

#add standard coordinates to make plotting easier
df_arable_with_OBCs_values <- cbind(df_arable_with_OBCs_values, crds(df_arable_with_OBCs, df=TRUE, list=FALSE))
df_arable_with_OBCs_values
```

```{r add_depth_and_year}
df_depth_10 <- df_arable_with_OBCs_values|>
  mutate(depth = 10, 
         recording_year = 1995)

df_depth_60 <- df_arable_with_OBCs_values|>
  mutate(depth = 60, 
         recording_year = 1995)
```

# Prepare the dataframe for modelling 

```{r standardize}
# Prepare parameters for standardizing the covariates
scaling_parameters <- read.csv(file= "scaling_parameters.csv")
scaling_parameters
# Convert scaling_df to named vectors for easy access
means <- setNames(scaling_parameters$mean, scaling_parameters$variable)
sds   <- setNames(scaling_parameters$sd, scaling_parameters$variable)

numeric_variables <- df_depth_10|>
  keep(is.numeric) 

# Identify which columns in df to scale (i.e., those in scaling_df$variable)
vars_to_scale <- intersect(names(numeric_variables), scaling_parameters$variable)

# Scale the variables
df_depth_10_scaled <- df_depth_10 %>%
  mutate(across(all_of(vars_to_scale), ~ (. - means[cur_column()]) / sds[cur_column()]))

df_depth_60_scaled <- df_depth_60 %>%
  mutate(across(all_of(vars_to_scale), ~ (. - means[cur_column()]) / sds[cur_column()]))
```



```{r one_hot_encode}
recipe_obj <- recipe(~ ., data = df_depth_10_scaled) %>%
  step_dummy(all_nominal(), one_hot = TRUE) %>%
  prep()

one_hot_df_10 <- bake(recipe_obj, new_data = NULL)
one_hot_df_10

recipe_obj <- recipe(~ ., data = df_depth_60_scaled) %>%
  step_dummy(all_nominal(), one_hot = TRUE) %>%
  prep()

one_hot_df_60 <- bake(recipe_obj, new_data = NULL)
one_hot_df_60
```



```{r remove_irelevant_var}
# Store x,y to join predictions with
coordinates <- one_hot_df_10|>
  dplyr::select (x, y)
# remove x.y to create prediction dfs 
x_map_10 <- one_hot_df_10|>
  dplyr::select (!c(x, y))
x_map_60 <- one_hot_df_60|>
  dplyr::select (!c(x, y))
```


```{r compare_columns}
# compare columns in the x that models are trained on
x_test <- read.csv(file='../data/prepared_data/x_test.csv') 
x_test

# see if there is any columns missing (this is identical for 10 and 60)
setdiff(colnames(x_test),colnames(x_map_10)) # no additional columns in x_test that are not already in x_map
set_diff_extra_map <- setdiff(colnames(x_map_10),colnames(x_test)) # columns missing from x_test (all categorical, therefore, model does not know what to do with them. they need to be removed)]
set_diff_extra_map

# Remove rows with impossible values and columns that don't exist in training data
x_map_filtered_10 <- x_map_10|>
  filter(across(set_diff_extra_map, ~ .!=1))|>
  dplyr::select(!set_diff_extra_map)
x_map_filtered_10  
x_map_filtered_60 <- x_map_60|>
  filter(across(set_diff_extra_map, ~ .!=1))|>
  dplyr::select(!set_diff_extra_map)


setdiff(colnames(x_map_filtered_10),colnames(x_test))

# reorder correctly
x_map_subset_10<-x_map_filtered_10[names(x_test)]
x_map_subset_10
x_map_subset_60<-x_map_filtered_60[names(x_test)]
x_map_subset_60
```



```{r ready_matrix}
# transform into matrix
x_map_matrix_10 <- x_map_subset_10|>
  as.matrix()
x_map_matrix_60 <- x_map_subset_60|>
  as.matrix()
```

```{r save_df}
 write.csv(x_map_matrix_10, file = '../data/prepared_data/x_map_10.csv', row.names=FALSE)
 write.csv(x_map_matrix_60, file = '../data/prepared_data/x_map_60.csv', row.names=FALSE)
 write.csv(x_map_matrix, file = '../data/prepared_data/coordinates_map.csv', row.names=FALSE)
```

