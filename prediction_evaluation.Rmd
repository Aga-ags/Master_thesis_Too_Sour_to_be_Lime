---
title: "Prediction evaluation"
author: "Agnieszka Kubica"
date: "`r Sys.Date()`"
output: html_document
---

```{r}
library(tidyverse)
library(rlang)
```



# Functions
```{r}
# Pivot to longer 
pivot_to_pred_model <- function(df){
  df |>
  pivot_longer(
    cols = -c(1:2),
    #names_pattern = "(pH|lime)_(.*)$",
    #names_to = c("predicted", "model"))
    names_pattern  =  "^(.*)\\.(pH|lime)$", #final_pH_full_imp.pH
    names_to = c("model", "predicted"))
}

plot_observed_against_predicted <- function(df){
  
}
```

```{r}
# Calculate performace metrics
metrics_by_model <-function(df_long, outcome){
  observed_col <- sym(outcome)
  
  df_long %>%
    filter(predicted == outcome)|>
    group_by(model) %>%
    summarise(
      MAE  = mean(abs(!!observed_col - value), na.rm = TRUE),
      MSE  = mean((!!observed_col - value)^2, na.rm = TRUE),
      RMSE = sqrt(mean((!!observed_col - value)^2, na.rm = TRUE)),
      R2   = 1 - sum((!!observed_col - value)^2, na.rm = TRUE) / sum((!!observed_col - mean(!!observed_col, na.rm = TRUE))^2, na.rm = TRUE),
             
      `MSE zero values` = {
        y_true <- pull(cur_data(), !!observed_col)
        y_pred <- pull(cur_data(), value)
        if (sum(y_true == 0, na.rm = TRUE) > 0) {
          mean((y_pred[y_true == 0] - y_true[y_true == 0])^2, na.rm = TRUE)
        } else {
          0
        }
      },
      
      `MSE non-zero values` = {
        y_true <- pull(cur_data(), !!observed_col)
        y_pred <- pull(cur_data(), value)
        if (sum(y_true > 0, na.rm = TRUE) > 0) {
          mean((y_pred[y_true > 0] - y_true[y_true > 0])^2, na.rm = TRUE)
        } else {
          0
        }
      },
      
      `Zero-inflated MSE` = (`MSE zero values` + `MSE non-zero values`) / 2
    )|>
    print()
}
```

```{r funtion_plot_observed_predicted}
plot_observed_predicted <-function(df_long, outcome){
  plot <- df_long |>
    filter(predicted == outcome)|>
    ggplot(aes(x = !!sym(outcome), y = value)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  facet_wrap(~ model) +
  labs(
    title = paste("Relationship between observed and predicted", outcome, "values"),
    x = paste("Actual", outcome),
    y = paste("Predicted", outcome)
  ) +
  theme_minimal()
  
  print(plot)
}
```

```{r functions_plot_residuals}
plot_residuals <-function(df_long, outcome){
  df <- df_long|>
   filter(predicted == outcome)|>
    mutate(residuals = !!sym(outcome) - value)
  
  res_dist1 <- ggplot(df, aes(x = value, y = residuals)) +
  geom_point(color = "darkorange2") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = paste("Residuals vs Fitted for", outcome), x = "Predicted", y = "Residuals") +
  theme_minimal()+
     facet_wrap(~ model)
  
  res_dist2 <- ggplot(df, aes(x = residuals)) +
  geom_histogram(bins = 30, fill = "darkorange2",color = "black") +
  labs(title = paste("Distribution of residuals for", outcome), x = "Residual", y = "Count") +
  theme_minimal()+
     facet_wrap(~ model)
  
  print(res_dist1)
  print(res_dist2)
}


```

```{r compare_output_distributions}

plot_compare_output_distributions <-function(df_long, outcome){
  df <- df_long|>
   filter(predicted == outcome)
  
    predicted_distribution_validation_data <- ggplot(df, aes())+
    geom_histogram(aes(x = !!sym(outcome)), fill = "black")+
    geom_histogram(aes(x = value), fill = "red", alpha = 0.7)+
    labs(title = paste("Distribution of predicted", outcome, "in validation data"), x = "Observed values") +
  theme_minimal()+
      facet_wrap(~ model)
    
  print(predicted_distribution_validation_data)
  
}

```

```{r}
analyze_output <- function(df_long, outcome){
  metrics_by_model(df_long, outcome)
  plot_observed_predicted(df_long, outcome)
  plot_residuals(df_long, outcome)
  plot_compare_output_distributions(df_long, outcome)
}
```

# Load data

```{r}
df_both <- read.csv(file="./tuning_results/final_both_validation.csv")
df_both

df_long_both <- pivot_to_pred_model(df_both)
df_long_both
```

```{r load_pH_base}
# pH only
df_pH <- read.csv(file="./tuning_results/final_pH_validation.csv")
df_pH

df_long_pH <- pivot_to_pred_model(df_pH)

#df_long_pH$model <- ordered(df_long_pH$model, levels =c("only_H2O", "pH_imp", "H20_and_lime_65", "imp_and_lime_65", "imp_and_lime_65_0_2", "full_imp", "only_h2o_ca2cl_all_lime", "only_ca2cl_all_lime"))
#df_long_pH

#df_pH_test <- read.csv(file="./tuning_results/predictions_test_pH.csv")
#df_pH_test

#df_long_pH_test <- pivot_to_pred_model(df_pH_test)

#df_long_pH_test$model <- ordered(df_long_pH_test$model, levels =c("only_H2O", "pH_imp", "H20_and_lime_65", "imp_and_lime_65", "imp_and_lime_65_0_2", "full_imp", "only_h2o_ca2cl_all_lime", "only_ca2cl_all_lime"))
#df_long_pH_test
```

```{r}
# pH weigths and dropout experiments 

# no weights

df_pH_no_weights <- read.csv(file="./tuning_results/predictions_validation_pH_no_weights.csv")
df_pH_no_weights

df_long_pH_no_weights <- pivot_to_pred_model(df_pH_no_weights)


# no dropout

df_pH_no_dropout <- read.csv(file="./tuning_results/predictions_validation_pH_no_dropout.csv")
df_pH_no_dropout

df_long_pH_no_dropout <- pivot_to_pred_model(df_pH_no_dropout)

# no either 

df_pH_no_either <- read.csv(file="./tuning_results/predictions_validation_pH_no_either.csv")
df_pH_no_either

df_long_pH_no_either <- pivot_to_pred_model(df_pH_no_either)
```

```{r load_experiments_ph}
df_pH_experiments <- read.csv(file="./tuning_results/predictions_validation_pH_try_2.csv")
df_pH_experiments

df_long_pH_experiments <- pivot_to_pred_model(df_pH_experiments)

```

```{r load_lime_base}

df_lime <- read.csv(file="./tuning_results/predictions_validation_lime_try_2.csv")
df_lime

df_lime_long <- pivot_to_pred_model(df_lime)
df_lime_long
```


```{r load_both_df}
df_both <- read.csv(file="./tuning_results/predictions_validation_both_try2.csv")
df_both

df_both_long <- df_both |>
  pivot_longer(
    cols = -c(`pH`, `lime`),  # keep observed columns
    names_to = "model_pred", 
    values_to = "value"
  ) %>%
  separate(model_pred, into = c("model", "predicted"), sep = "\\.", extra = "merge")

df_both_long
```
```{r}
# Regularization

df_regularization <- read.csv(file="./tuning_results/validation_with_regulralization.csv")
df_regularization

df_regularization_long <- pivot_to_pred_model(df_regularization)
df_regularization_long
```


# Perform data evaluation 


```{r}
analyze_output(df_long_both, outcome = "pH")
analyze_output(df_long_both, outcome = "lime")
```

```{r}
analyze_output(df_long_pH, outcome = "pH")
```


```{r ph_analysis, fig.width = 10}
# data evaluation on pH models with weights and dropout: 

analyze_output(df_long_pH, outcome = "pH")

analyze_output(df_long_pH_no_weights, outcome = "pH")

analyze_output(df_long_pH_no_dropout, outcome = "pH")

analyze_output(df_long_pH_no_either, outcome = "pH")

```

In terms of MSE and MAE: having all pH imputed is the best when weights are included and only H20 seems to be better with no weights. Including weights leads to a better perdiction and dropout also marginally improves the values. The effect much better when looking at distributions of full imp with and without dropout. 


```{r}
# Find performace matrics for base pH model
pH_metrics <- metrics_by_model(df_long_pH_experiments, outcome = "pH")
pH_metrics

plot_observed_predicted(df_long_pH_experiments, outcome = "pH")

plot_residuals(df_long_pH_experiments, outcome = "pH")

plot_compare_output_distributions(df_long_pH_experiments, outcome = "pH")
```
```{r}
# lime
analyze_output(df_lime_long,  outcome = "lime")

```
```{r regularization_lime}


analyze_output(df_regularization_long,  outcome = "lime")

```

```{r}
# both
analyze_output(df_both_long,  outcome = "pH")
analyze_output(df_both_long,  outcome = "lime")


# both_pH_imp_try2 is named incorrectly -> it's full imputation 
```

both_imp_and_lime_65_0_2_try2 performs best -> suggests that imputting all pH and lime from pH and classes 0-2 are the better appoach. 
