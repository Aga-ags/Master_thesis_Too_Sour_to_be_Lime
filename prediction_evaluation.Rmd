---
title: "Prediction evaluation"
author: "Agnieszka Kubica"
date: "`r Sys.Date()`"
output: html_document
---

```{r}
library(tidyverse)
library(rlang)
library(xtable) # export tables to LaTeX
```



# Functions
```{r}
# Pivot to longer 
pivot_to_pred_model <- function(df){
  df |>
  pivot_longer(
    cols = -c(1:2),
    #names_pattern = "(pH|lime)_(.*)$",
    #names_to = c("predicted", "model"))
    names_pattern  =  "^(.*)\\.(pH|lime)$", #final_pH_full_imp.pH
    names_to = c("model", "predicted"))
}

plot_observed_against_predicted <- function(df){
  
}
```

```{r}
# Calculate performance metrics
metrics_by_model <-function(df_long, outcome){
  observed_col <- sym(outcome)
  
  df_long %>%
    filter(predicted == outcome)|>
    group_by(model) %>%
    summarise(
      RMSE = sqrt(mean((!!observed_col - value)^2)),
      MAE  = mean(abs(!!observed_col - value)),
      MBE = mean(value - !!observed_col),
             
      `MSE zero values` = {
        y_true <- pull(cur_data(), !!observed_col)
        y_pred <- pull(cur_data(), value)
        if (sum(y_true + 0.5062935 < 0.0001, na.rm = TRUE) > 0) { # Zero values are not 0, because lime is standardized. Equivalent value of - 0.5062935 is used, however, due to rounding errors a equality is substituted to < with 0.0001 tolarance
          mean((y_pred[y_true + 0.5062935 < 0.0001] - y_true[y_true + 0.5062935 < 0.0001])^2, na.rm = TRUE)
        } else {
          0
        }
      },
      
      `MSE non-zero values` = {
        y_true <- pull(cur_data(), !!observed_col)
        y_pred <- pull(cur_data(), value)
        if (sum(y_true > (-0.5062935 + 0.0001), na.rm = TRUE) > 0) { # similar situation with standarization and rounding error
          mean((y_pred[y_true > (-0.5062935 + 0.0001)] - y_true[y_true > (-0.5062935 + 0.0001)])^2, na.rm = TRUE)
        } else {
          0
        }
      },
      
      `Zero-inflated MSE` = (`MSE zero values` + `MSE non-zero values`) / 2
    )|>
    print(n = nrow(.))
}
```

```{r funtion_plot_observed_predicted}
plot_observed_predicted <-function(df_long, outcome){
  plot <- df_long |>
    filter(predicted == outcome)|>
    ggplot(aes(x = !!sym(outcome), y = value)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  facet_wrap(~ model) +
  labs(
    title = paste("Relationship between observed and predicted", outcome, "values"),
    x = paste("Actual", outcome),
    y = paste("Predicted", outcome)
  ) +
  theme_minimal()
  
  print(plot)
}
```

```{r functions_plot_residuals}
plot_residuals <-function(df_long, outcome){
  df <- df_long|>
   filter(predicted == outcome)|>
    mutate(residuals = !!sym(outcome) - value)
  
  res_dist1 <- ggplot(df, aes(x = value, y = residuals)) +
  geom_point(color = "darkorange2") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = paste("Residuals vs Fitted for", outcome), x = "Predicted", y = "Residuals") +
  theme_minimal()+
     facet_wrap(~ model)
  
  res_dist2 <- ggplot(df, aes(x = residuals)) +
  geom_histogram(bins = 30, fill = "darkorange2",color = "black") +
  labs(title = paste("Distribution of residuals for", outcome), x = "Residual", y = "Count") +
  theme_minimal()+
     facet_wrap(~ model)
  
  print(res_dist1)
  print(res_dist2)
}


```

```{r compare_output_distributions}

plot_compare_output_distributions <-function(df_long, outcome){
  df <- df_long|>
   filter(predicted == outcome)
  
    predicted_distribution_validation_data <- ggplot(df, aes())+
    geom_histogram(aes(x = !!sym(outcome)), fill = "black")+
    geom_histogram(aes(x = value), fill = "red", alpha = 0.7)+
    labs(title = paste("Distribution of predicted", outcome, "in validation data"), x = "Observed values") +
  theme_minimal()+
      facet_wrap(~ model)
    
  print(predicted_distribution_validation_data)
  
}
```

```{r}
# This function performs the evaluation of the model performance per output
analyze_output <- function(df_long, outcome){
  metrics_by_model(df_long, outcome)
  plot_observed_predicted(df_long, outcome)
  plot_residuals(df_long, outcome)
  plot_compare_output_distributions(df_long, outcome)
}
```


# Choice of imputation strategy and weights inclusion
```{r load_imp_data}
df_imp_pH <- read.csv(file="./tuning_results/pH_imputation_scenario_validation.csv")|>
  pivot_to_pred_model()
df_imp_pH

df_imp_lime <- read.csv(file="./tuning_results/lime_imputation_scenario_validation.csv")|>
  pivot_to_pred_model()
df_imp_lime

df_imp_both <- read.csv(file="./tuning_results/both_imputation_scenario_validation.csv")|>
  pivot_to_pred_model()
df_imp_both

# Order the model column to have the same order on all graphs
df_imp_pH$model <- ordered(df_imp_pH$model, levels =c("pH_no_site_H2O_no_weights", "pH_no_site_no_weights", "pH_full_imp_no_weights", "pH_no_site_H2O", "pH_no_site", "pH_full_imp"))

df_imp_lime$model <- ordered(df_imp_lime$model, levels =c("lime_no_lime_imputation_no_weights","lime_no_lime_imputation_from_lime_classes_no_weights", "lime_no_lime_imputation_from_3_5_classes_no_weights", "lime_full_imp_no_weights", "lime_no_lime_imputation","lime_no_lime_imputation_from_lime_classes", "lime_no_lime_imputation_from_3_5_classes", "lime_full_imp"))


df_imp_both$model <- ordered(df_imp_both$model, levels =c(
  "both_no_imputation_no_weights",
  "both_no_lime_imputation_site_no_weights",
  "both_no_lime_imputation_no_weights",
  "both_no_lime_imputation_from_lime_classes_H2O_site_no_weights",
  "both_no_lime_imputation_from_lime_classes_site_no_weights", 
  "both_no_lime_imputation_from_lime_classes_no_weights",
  "both_no_lime_imputation_from_3_5_classes_H2O_site_no_weights", 
  "both_no_lime_imputation_from_3_5_classes_site_no_weights",
  "both_no_lime_imputation_from_3_5_classes_no_weights", 
  "both_no_site_H2O_no_weights", 
  "both_no_site_no_weights",
  "both_full_imp_no_weights", 
  "both_no_imputation",
  "both_no_lime_imputation_site",
  "both_no_lime_imputation",
  "both_no_lime_imputation_from_lime_classes_H2O_site",
  "both_no_lime_imputation_from_lime_classes_site", 
  "both_no_lime_imputation_from_lime_classes",
  "both_no_lime_imputation_from_3_5_classes_H2O_site", 
  "both_no_lime_imputation_from_3_5_classes_site",
  "both_no_lime_imputation_from_3_5_classes", 
  "both_no_site_H2O", 
  "both_no_site",
  "both_full_imp"
))
```


```{r imp_weights_pH}
# compare the model perfomance across imputations for univariate pH model:
analyze_output(df_imp_pH, outcome = "pH")
```

```{r imp_weights_lime}
# compare the model perfomance across imputations for univariate lime content model:
analyze_output(df_imp_lime, outcome = "lime")
```


```{r imp_weights_both}
# compare the model perfomance across imputations for univariate lime content model:
analyze_output(df_imp_both, outcome = "pH")
analyze_output(df_imp_both, outcome = "lime")
```


```{r}
xtable(metrics_by_model(df_imp_both, outcome = "pH"), digits = 3)
xtable(metrics_by_model(df_imp_both, outcome = "lime"), digits = 3)
```

## Overfitting

```{r}
overfit_df <- read.csv(file="./tuning_results/overfitting_parameter_tuning.csv")|>
  pivot_to_pred_model()

analyze_output(overfit_df, outcome = "pH")
analyze_output(overfit_df, outcome = "lime")
```
```{r}
overfit_df <- read.csv(file="./tuning_results/zero_mse_parameter_tuning.csv")|>
  pivot_to_pred_model()

analyze_output(overfit_df, outcome = "lime")
```
```{r}
lambda_df <- read.csv(file="./tuning_results/lambda_parameter_tuning.csv")|>
  pivot_to_pred_model()
lambda_df
analyze_output(lambda_df, outcome = "lime")

mse_of_rule_points_negative_incorrect(lambda_df)
```
Best perfomance: lanbda 0.001, then, 0.5



# Evaluation of consistency 

```{r extract_best_models_df}

# Prepare parameters for de-scaling pH and lime content
scaling_parameters <- read.csv(file= "scaling_parameters.csv")

ph_mean <- scaling_parameters$mean[scaling_parameters$variable=="pH"]
ph_std <- scaling_parameters$sd[scaling_parameters$variable=="pH"]
lime_mean <- scaling_parameters$mean[scaling_parameters$variable=="lime"]
lime_std <- scaling_parameters$sd[scaling_parameters$variable=="lime"]

# Create base dataframe with observed values to join with
df_best_models <- read.csv(file= "../data/prepared_data/y_valid.csv")|>
  dplyr::select(pH, lime) |>
  mutate(pH = (pH * ph_std) + ph_mean,
         lime = (lime * lime_std) + lime_mean,
         model = "Observed values")
# add index to later join by
df_best_models$id <- seq.int(nrow(df_best_models))



# Extract the best model:
extract_best_model <- function(file_name, best_model_name, rename_to, df_to_join){
  ph_col <- paste0(best_model_name, ".pH")
  lime_col <- paste0(best_model_name, ".lime")
  
  df <-  read.csv(file= paste0("./tuning_results/", file_name, ".csv"))
  # add index to later join by
  df$id <- seq.int(nrow(df))  
  
  df <- df |>
    dplyr::select(id, ph_col, lime_col)|>
    rename(pH = ph_col, 
           lime = lime_col)|>
    # de-scale the predicted values
    mutate(pH = (pH * ph_std) + ph_mean,
         lime = (lime * lime_std) + lime_mean,
         model = rename_to)
  
  df_to_join <- dplyr::bind_rows(df_to_join, df)
  return(df_to_join)
}

df_best_models <- extract_best_model("lambda_parameter_tuning", best_model_name = "both_no_3_5_classes_dropout_penatly_1", rename_to = "both_penalty_1", df_best_models)
df_best_models <- extract_best_model("lambda_parameter_tuning", best_model_name = "both_no_3_5_classes_dropout_penatly_075", rename_to = "both_penalty_0_75", df_best_models)
df_best_models <- extract_best_model("lambda_parameter_tuning", best_model_name = "both_no_3_5_classes_dropout_penatly_05", rename_to = "both_penalty_0_5",df_best_models)
df_best_models <- extract_best_model("lambda_parameter_tuning", best_model_name = "both_no_3_5_classes_dropout_penatly_025", rename_to = "both_penalty_0_25",df_best_models)
df_best_models <- extract_best_model("lambda_parameter_tuning", best_model_name = "both_no_3_5_classes_dropout_penatly_01", rename_to = "both_penalty_0_1",df_best_models)
df_best_models <- extract_best_model("lambda_parameter_tuning", best_model_name = "both_no_3_5_classes_dropout_penatly_001", rename_to = "both_penalty_0_01",df_best_models)
df_best_models <- extract_best_model("lambda_parameter_tuning", best_model_name = "both_no_3_5_classes_dropout_penatly_0001", rename_to = "both_penalty_0_001",df_best_models)
```


```{r confusion_matrix}
# Make "confusion matrices" of each model

make_ph_lime_confusion_matrix <- function(df, model_name) {
  # Categorize each row
  df <- df %>%
    filter(model == model_name) %>%
    mutate(
      pH_group = ifelse(pH < 6.5, "<6.5", "≥6.5"),
      lime_group = ifelse(lime > 0.0001, "lime > 0", "lime ≤ 0")
    )
  
  # Create contingency table
  count_table <- df %>%
    count(pH_group, lime_group) %>%
    tidyr::complete(pH_group, lime_group, fill = list(n = 0))  # ensure all combinations exist
  
  # Total for percentages
  total <- sum(count_table$n)
  
  # Create matrix with both count and percentage
  formatted_matrix <- count_table %>%
    mutate(label = sprintf("%d (%.1f%%)", n, 100 * n / total)) %>%
    select(pH_group, lime_group, label) %>%
    pivot_wider(names_from = lime_group, values_from = label) %>%
    column_to_rownames("pH_group")
  
  return(formatted_matrix)
}

make_ph_lime_confusion_matrix(df_best_models, model_name = "Observed values")  
make_ph_lime_confusion_matrix(df_best_models, model_name = "both_no_3_5_classes_dropout_penatly_05") 

```


```{r metrics_of_ph_65}
# Should negative predictions of lime be considered here? If we consider them correct, the error increases 

mse_of_rule_points_negative_incorrect <- function(model_df){
   metrics <- model_df|>
    filter(pH < 6.5)|>
     group_by(model)|>
     summarise(
      RMSE = sqrt(mean((lime - 0)^2)), # we assume actual value is 0
      MAE  = mean(abs(lime  - 0))
    )
  return(metrics)
}

mse_of_rule_points_negative_correct <- function(model_df){
   metrics <- model_df|>
    filter(pH < 6.5)|>
    filter(lime > 0.0001)|> # filter only positive lime predictions
     group_by(model)|>
     summarise(
      RMSE = sqrt(mean((lime - 0)^2)), # we assume actual value is 0
      MAE  = mean(abs(lime  - 0)))
  return(metrics)
}
mse_of_rule_points_negative_incorrect(df_best_models)
mse_of_rule_points_negative_correct(df_best_models)
```
Again: 0.001 then 0.5 perform best

```{r histogram_consitency}
histogram_lime_below_65 <- function(model_df){
   plot <- model_df|>
    filter(pH < 6.5)|>
     ggplot(aes(x = lime))+
     geom_histogram()+
     facet_wrap(~model)+
     theme_minimal()
   return(plot)
}

histogram_lime_below_65(df_best_models)  
```

```{r}
predicted_against_predicted <- function(model_df){
   plot <- model_df|>
    mutate(low_pH = ifelse(pH < 6.5, TRUE, FALSE))|>
     ggplot(aes(x = pH, y = lime, colour = low_pH))+
     geom_point()+
     facet_wrap(~model)+
     theme_minimal()
   return(plot)
}

predicted_against_predicted(df_best_models)  
```
# Evaluating the consitency at different depths?




# Extract tables into latex 

```{r test_latex, results=tex}

xtable(metrics_by_model())

```
