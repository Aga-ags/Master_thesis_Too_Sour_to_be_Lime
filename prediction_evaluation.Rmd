---
title: "Prediction evaluation"
author: "Agnieszka Kubica"
date: "`r Sys.Date()`"
output: html_document
---

```{r}
library(tidyverse)
library(xtable) # export tables to LaTeX
library(forcats) # revalue factors
library(rlang)
library(scales)
library("ggpubr")
```


# Functions
## Functions for evaluating accuracy
```{r}
# Pivot to longer 
scaling_parameters <- read.csv(file= "scaling_parameters.csv")

ph_mean <- scaling_parameters$mean[scaling_parameters$variable=="pH"]
ph_std <- scaling_parameters$sd[scaling_parameters$variable=="pH"]
lime_mean <- scaling_parameters$mean[scaling_parameters$variable=="lime"]
lime_std <- scaling_parameters$sd[scaling_parameters$variable=="lime"]

pivot_to_pred_model <- function(df){
  df |>
  pivot_longer(
    cols = -c(1:2),
    names_pattern  =  "^(.*)\\.(pH|lime)$", #final_pH_full_imp.pH
    names_to = c("model", "predicted"))|>
    mutate(value = ifelse(predicted == "pH",
                        (value * ph_std) + ph_mean,
                        (value * lime_std) + lime_mean),
         pH = (pH * ph_std) + ph_mean,
         lime = (lime * lime_std) + lime_mean,
         model = as.factor(model))
}


plot_observed_against_predicted <- function(df){
  
}
```


```{r}
# Calculate performance metrics
metrics_by_model <-function(df_long, outcome){
  observed_col <- sym(outcome)
  
  df_long %>%
    filter(predicted == outcome)|>
    group_by(model) %>%
    summarise(
      RMSE = round(sqrt(mean((!!observed_col - value)^2)), digits = 3),
      MAE  = round(mean(abs(!!observed_col - value)), digits = 3),
      MBE = round(mean(value - !!observed_col), digits = 3),
             
      `RMSE zero values` = {
        y_true <- pull(cur_data(), !!observed_col)
        y_pred <- pull(cur_data(), value)
        if (sum(y_true < 0.0001, na.rm = TRUE) > 0) { # due to rounding errors a equality is substituted to < with 0.0001 tolerance
          round(sqrt(mean((y_pred[y_true  < 0.0001] - y_true[y_true  < 0.0001])^2, na.rm = TRUE)), digits = 3)
        } else {
          0
        }
      },
      
      `RMSE non-zero values` = {
        y_true <- pull(cur_data(), !!observed_col)
        y_pred <- pull(cur_data(), value)
        if (sum(y_true > ( 0.0001), na.rm = TRUE) > 0) { # similar situation with rounding error
          round(sqrt(mean((y_pred[y_true > ( 0.0001)] - y_true[y_true > (0.0001)])^2, na.rm = TRUE)), digits = 3)
        } else {
          0
        }
      },
      
      `Zero-inflated RMSE` = round((`RMSE zero values` + `RMSE non-zero values`) / 2, digits = 3)
    )|>
    print(n = nrow(.))
}
```

```{r funtion_plot_observed_predicted}
plot_observed_predicted <-function(df_long, outcome){
  plot <- df_long |>
    filter(predicted == outcome)|>
    ggplot(aes(x = !!sym(outcome), y = value)) +
  geom_point( alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  facet_wrap(~ model) +
  labs(
    x = paste("Observed", outcome),
    y = paste("Predicted", outcome)
  ) +
  theme_minimal()
  
  # Formatting label to contain unit for lime
  if(outcome == "lime") {
  plot <- plot +
    labs(
    x = paste("Observed", outcome, "content [%]"),
    y = paste("Predicted", outcome, "content [%]")
  )
}
  
  print(plot)
}
```

```{r functions_plot_residuals}
plot_residuals <-function(df_long, outcome){
  df <- df_long|>
   filter(predicted == outcome)|>
    mutate(residuals = !!sym(outcome) - value)
  
  res_dist1 <- ggplot(df, aes(x = value, y = residuals)) +
  geom_point(color = "darkorange2", alpha = 0.3) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = paste("Residuals vs Fitted for", outcome), x =  paste("Predicted", outcome), y = "Residuals") +
  theme_minimal()+
     facet_wrap(~ model)
 
  # Formatting label to contain unit for lime 
  if(outcome == "lime") {
    res_dist1 <- res_dist1 +
      labs(x = paste("Predicted", outcome, "content [%]"))
  }

  
  res_dist2 <- ggplot(df, aes(x = residuals)) +
  geom_histogram(bins = 30, fill = "darkorange2",color = "black") +
  labs(title = paste("Distribution of residuals for", outcome), x = "Residuals", y = "Count") +
  theme_minimal()+
     facet_wrap(~ model)
  
  print(res_dist1)
  print(res_dist2)
  return(res_dist1)
}
```

```{r compare_output_distributions}
plot_compare_output_distributions <- function(df_long, outcome) {
  df <- df_long |> filter(predicted == outcome)
  
  df_obs <- df |> 
    mutate(type = "Observed", val = !!sym(outcome))
  
  df_pred <- df |> 
    mutate(type = "Predicted", val = value)
  
  df_combined <- bind_rows(df_obs, df_pred)
  
  predicted_distribution_validation_data <- ggplot(df_combined, aes(x = val, fill = type)) +
    geom_histogram(position = "identity", alpha = 0.7, bins = 30) +
    scale_fill_manual(values = c("Observed" = "black", "Predicted" = "red")) +
    labs(
      x = outcome,
      fill = "Legend"
    ) +
    theme_minimal() +
    facet_wrap(~ model)
  
  # Formatting label to contain unit for lime
   if(outcome == "lime") {
    predicted_distribution_validation_data <- predicted_distribution_validation_data +
      labs(x = paste(outcome, "content [%]"))
   }
  
  print(predicted_distribution_validation_data)
}
```

```{r}
# This function performs the evaluation of the model performance per output
analyze_output <- function(df_long, outcome){
  metrics_by_model(df_long, outcome)
  plot_observed_predicted(df_long, outcome)
  plot_residuals(df_long, outcome)
  plot_compare_output_distributions(df_long, outcome)
}
```

## Functions for evaluating consitency 

```{r f_extract_best_model}
# Extract the best model:
extract_best_model <- function(file_name, best_model_name, rename_to, df_to_join){
  ph_col <- paste0(best_model_name, ".pH")
  lime_col <- paste0(best_model_name, ".lime")
  
  df <-  read.csv(file= paste0("./tuning_results/", file_name, ".csv"))
  # add index to later join by
  df$id <- seq.int(nrow(df))  
  
  df <- df |>
    dplyr::select(id, ph_col, lime_col)|>
    dplyr::rename(pH = ph_col, 
           lime = lime_col)|>
    # de-scale the predicted values
    mutate(pH = (pH * ph_std) + ph_mean,
         lime = (lime * lime_std) + lime_mean,
         model = rename_to)
  
  df_to_join <- dplyr::bind_rows(df_to_join, df)
  return(df_to_join)
}

extract_best_model_univariate <- function(file_name, best_model_name_ph, best_model_name_lime, rename_to, df_to_join){
  ph_col <- paste0(best_model_name_ph, ".pH")
  lime_col <- paste0(best_model_name_lime, ".lime")
  
  df <-  read.csv(file= paste0("./tuning_results/", file_name, ".csv"))
  # add index to later join by
  df$id <- seq.int(nrow(df))  
  
  df_pH <- df |>
    dplyr::select(id, ph_col)|>
    dplyr::rename(pH =  !!sym(ph_col))|>
    # de-scale the predicted values
    mutate(pH = (pH * ph_std) + ph_mean,
         model = rename_to)
  
  df_lime <- df |>
    dplyr::select(id, lime_col)|>
    dplyr::rename(lime = lime_col)|>
    # de-scale the predicted values
    mutate(lime = (lime * lime_std) + lime_mean)|>
    left_join(df_pH, by = "id")
  
  df_to_join <- dplyr::bind_rows(df_to_join, df_lime)
  return(df_to_join)}
```

```{r f_confusion_matrix}
make_ph_lime_confusion_matrix <- function(df, model_name) {
  # Categorize each row
  df <- df %>%
    filter(model == model_name) %>%
    mutate(
      pH_group = ifelse(pH < 6.5, "<6.5", "â‰¥6.5"),
      lime_group = ifelse(lime > 0.0001, "lime > 0", "lime = 0" )
    )
  
  # Create contingency table
  count_table <- df %>%
    count(pH_group, lime_group) %>%
    tidyr::complete(pH_group, lime_group, fill = list(n = 0))  # ensure all combinations exist
  
  # Total for percentages
  total <- sum(count_table$n)
  
  # Create matrix with both count and percentage
  formatted_matrix <- count_table %>%
    mutate(label = sprintf("%d (%.1f%%)", n, 100 * n / total)) %>%
    dplyr::select(pH_group, lime_group, label) %>%
    pivot_wider(names_from = lime_group, values_from = label) %>%
    column_to_rownames("pH_group")
  
  return(formatted_matrix)
}
```

```{r f_metrics_consistency}
metrics_of_rule_points <- function(model_df){
   metrics <- model_df|>
    filter(pH < 6.5)|>
     group_by(model)|>
     summarise(
      RMSE = round(sqrt(mean((lime - 0)^2)), digits = 3), # assume actual value is 0
      MAE  = round(mean(abs(lime  - 0)), digits = 3)
    )
  return(metrics)
}
```

```{r pred_vs_pred_plot}
# Plot of predicted lime and pH - showing the inferred relationship between the variables 
predicted_against_predicted <-function(model_df){
   plot <- model_df|>
    mutate(low_pH = ifelse(pH < 6.5, TRUE, FALSE))|>
     ggplot(aes(x = pH, y = lime, colour = low_pH))+
     geom_point()+
     labs(
    y ="Lime content [%]")+
     facet_wrap(~model)+
     theme_minimal()
  return(plot)
}
```

```{r f_histogram_consistency}
# histogram of predicted lime for values that are predicted below ph 6.5
histogram_lime_below_65 <- function(model_df){
   plot <- model_df|>
    filter(pH < 6.5)|>
     mutate(lime = lime + 0.1)|>
     ggplot(aes(x = lime))+
     geom_histogram(bins = 30)+
     facet_wrap(~model)+
     theme_minimal()+
       labs(
    x ="Lime content [%]",
     y ="Count [log]") +
     scale_y_continuous(trans = pseudo_log_trans(base = 2),
  breaks = c(2, 8, 32, 128))
   return(plot)
}
```


# Choice of imputation strategy and weights inclusion
```{r load_imp_data}
# load models with different imputation strategies 
df_imp_pH <- read.csv(file="./tuning_results/pH_imputation_scenario_validation.csv")|>
  pivot_to_pred_model()

df_imp_lime <- read.csv(file="./tuning_results/lime_imputation_scenario_validation.csv")|>
  pivot_to_pred_model()

df_imp_both <- read.csv(file="./tuning_results/both_imputation_scenario_validation.csv")|>
  pivot_to_pred_model()

# Order the model column to have the same order on all graphs
df_imp_pH$model <- ordered(df_imp_pH$model, levels =c("pH_no_site_H2O_no_weights", "pH_no_site_no_weights", "pH_full_imp_no_weights", "pH_no_site_H2O", "pH_no_site", "pH_full_imp"))

df_imp_lime$model <- ordered(df_imp_lime$model, levels =c("lime_no_lime_imputation_no_weights","lime_no_lime_imputation_from_lime_classes_no_weights", "lime_no_lime_imputation_from_3_5_classes_no_weights", "lime_full_imp_no_weights", "lime_no_lime_imputation","lime_no_lime_imputation_from_lime_classes", "lime_no_lime_imputation_from_3_5_classes", "lime_full_imp"))


df_imp_both$model <- ordered(df_imp_both$model, levels =c(
  "both_no_imputation_no_weights",
  "both_no_lime_imputation_site_no_weights",
  "both_no_lime_imputation_no_weights",
  "both_no_lime_imputation_from_lime_classes_H2O_site_no_weights",
  "both_no_lime_imputation_from_lime_classes_site_no_weights", 
  "both_no_lime_imputation_from_lime_classes_no_weights",
  "both_no_lime_imputation_from_3_5_classes_H2O_site_no_weights", 
  "both_no_lime_imputation_from_3_5_classes_site_no_weights",
  "both_no_lime_imputation_from_3_5_classes_no_weights", 
  "both_no_site_H2O_no_weights", 
  "both_no_site_no_weights",
  "both_full_imp_no_weights", 
  "both_no_imputation",
  "both_no_lime_imputation_site",
  "both_no_lime_imputation",
  "both_no_lime_imputation_from_lime_classes_H2O_site",
  "both_no_lime_imputation_from_lime_classes_site", 
  "both_no_lime_imputation_from_lime_classes",
  "both_no_lime_imputation_from_3_5_classes_H2O_site", 
  "both_no_lime_imputation_from_3_5_classes_site",
  "both_no_lime_imputation_from_3_5_classes", 
  "both_no_site_H2O", 
  "both_no_site",
  "both_full_imp"
))
```


```{r imp_weights_pH}
# compare the model perfomance across imputations for univariate pH model:
analyze_output(df_imp_pH, outcome = "pH")
```
When weights are not included: more imputation leads to a better prediction... But when weights are included the prediction gets worse. But in reality the size of the error differences is quite small (on average prediction being 0.40 vs 0.46). All models struggle with the inflated predicted values for low pH values (below 6.5).


```{r imp_weights_lime}
# compare the model perfomance across imputations for univariate lime content model:
analyze_output(df_imp_lime, outcome = "lime")
```
Again, the differences in model performance are not large. The biggest diffrences are in the ability of the model to predict zero lime values - here clearly no-imputation from classes 3-5 is the best with and without weights, however, that does come with a cost of a worse non-zero values prediction (particuralyvery high lime values are being predicted as zero). 

```{r imp_weights_both}
# compare the model perfomance across imputations for univariate lime content model:
analyze_output(df_imp_both, outcome = "pH")
analyze_output(df_imp_both, outcome = "lime")
```
Here: no imputation performs best for pH... but if we look at predicted vs observed it shows almost a flat line - not good. Similar flatness occurs in the full imputation with weights. 


Imputation without 3-5 lime site classes was chosen with inclusion of weights. 



## Overfitting

```{r}
overfit_df <- read.csv(file="./tuning_results/overfitting_parameter_tuning.csv")|>
  pivot_to_pred_model()

analyze_output(overfit_df, outcome = "pH")
analyze_output(overfit_df, outcome = "lime")
```


```{r residuals_best_models, fig.width = 8, fig.height=8}
a <- plot_residuals(overfit_df, outcome = "pH")
b <- plot_residuals(overfit_df, outcome = "lime")

ggarrange(
  a, b, labels = c("A", "B"),
  ncol = 1, nrow = 2
  )

ggsave(file = "../Figures/overfitting_residuals.pdf")

```

# Lambda - impact on accuracy and consistency 
## Linear penalty
```{r}
lambda_df_lin <- read.csv(file="./tuning_results/lambda_parameter_tuning.csv")|>
  pivot_to_pred_model()
lambda_df_lin

lambda_df_lin$model <-  fct_recode(lambda_df_lin$model,  
                                   "No penalty" = "both_no_lime_imputation_from_3_5_classes", 
                                   "Linear penalty, lambda 1" = "both_no_3_5_classes_penatly_1",  
                                   "Linear penalty, lambda 0.5" = "both_no_3_5_classes_penatly_05", 
                                   "Linear penalty, lambda 0.1" = "both_no_3_5_classes_penatly_01", 
                                   "Linear penalty, lambda 0.05" = "both_no_3_5_classes_penatly_005", 
                                   "Linear penalty, lambda 0.01" = "both_no_3_5_classes_penatly_001", 
                                   "Linear penalty, lambda 0.005" = "both_no_3_5_classes_penatly_0005", 
                                   "Linear penalty, lambda 0.001" = "both_no_3_5_classes_penatly_0001")

lambda_df_lin$model <- ordered(lambda_df_lin$model, levels =c("No penalty","Linear penalty, lambda 1", "Linear penalty, lambda 0.5", "Linear penalty, lambda 0.1", "Linear penalty, lambda 0.05","Linear penalty, lambda 0.01", "Linear penalty, lambda 0.005", "Linear penalty, lambda 0.001"))


analyze_output(lambda_df_lin, outcome = "pH")
analyze_output(lambda_df_lin, outcome = "lime")
```




```{r }
# Transform the models into the format for consistency evaluation 
# Prepare parameters for de-scaling pH and lime content
scaling_parameters <- read.csv(file= "scaling_parameters.csv")

ph_mean <- scaling_parameters$mean[scaling_parameters$variable=="pH"]
ph_std <- scaling_parameters$sd[scaling_parameters$variable=="pH"]
lime_mean <- scaling_parameters$mean[scaling_parameters$variable=="lime"]
lime_std <- scaling_parameters$sd[scaling_parameters$variable=="lime"]

# Create base dataframe with observed values to join with
df_conistency_lin <- read.csv(file= "../data/prepared_data/y_valid.csv")|>
  dplyr::select(pH, lime) |>
  mutate(pH = (pH * ph_std) + ph_mean,
         lime = (lime * lime_std) + lime_mean,
         model = "Observed values")
# add index to later join by
df_conistency_lin$id <- seq.int(nrow(df_conistency_lin))

df_conistency_lin <- extract_best_model("lambda_parameter_tuning", best_model_name = "both_no_lime_imputation_from_3_5_classes", rename_to = "No penalty", df_conistency_lin)
df_conistency_lin <- extract_best_model("lambda_parameter_tuning", best_model_name = "both_no_3_5_classes_penatly_1", rename_to = "Linear penalty, lambda 1", df_conistency_lin)
df_conistency_lin <- extract_best_model("lambda_parameter_tuning", best_model_name = "both_no_3_5_classes_penatly_05", rename_to = "Linear penalty, lambda 0.5", df_conistency_lin)
df_conistency_lin <- extract_best_model("lambda_parameter_tuning", best_model_name = "both_no_3_5_classes_penatly_01", rename_to = "Linear penalty, lambda 0.1",df_conistency_lin)
df_conistency_lin <- extract_best_model("lambda_parameter_tuning", best_model_name = "both_no_3_5_classes_penatly_005", rename_to = "Linear penalty, lambda 0.05",df_conistency_lin)
df_conistency_lin <- extract_best_model("lambda_parameter_tuning", best_model_name = "both_no_3_5_classes_penatly_001", rename_to = "Linear penalty, lambda 0.01",df_conistency_lin)
df_conistency_lin <- extract_best_model("lambda_parameter_tuning", best_model_name = "both_no_3_5_classes_penatly_0005", rename_to = "Linear penalty, lambda 0.005",df_conistency_lin)
df_conistency_lin <- extract_best_model("lambda_parameter_tuning", best_model_name = "both_no_3_5_classes_penatly_0001", rename_to = "Linear penalty, lambda 0.001",df_conistency_lin)
df_conistency_lin

# Confusion matrix
make_ph_lime_confusion_matrix(df_conistency_lin, model_name = "No penalty")  
make_ph_lime_confusion_matrix(df_conistency_lin, model_name = "Linear penalty, lambda 1") 
make_ph_lime_confusion_matrix(df_conistency_lin, model_name = "Linear penalty, lambda 0.5") 
make_ph_lime_confusion_matrix(df_conistency_lin, model_name = "Linear penalty, lambda 0.1")
make_ph_lime_confusion_matrix(df_conistency_lin, model_name = "Linear penalty, lambda 0.05")
make_ph_lime_confusion_matrix(df_conistency_lin, model_name = "Linear penalty, lambda 0.01")
make_ph_lime_confusion_matrix(df_conistency_lin, model_name = "Linear penalty, lambda 0.005")
make_ph_lime_confusion_matrix(df_conistency_lin, model_name = "Linear penalty, lambda 0.001")

# Metrics
metrics_of_rule_points(df_conistency_lin)

# Histograms
histogram_lime_below_65(df_conistency_lin) 

# Scatterplot of pH against lime
predicted_against_predicted(df_conistency_lin) 
```


## Quadratic penalty

```{r}
lambda_df_sqr <- read.csv(file="./tuning_results/lambda_parameter_tuning_squared.csv")|>
  pivot_to_pred_model()
lambda_df_sqr

lambda_df_sqr$model <-  fct_recode(lambda_df_sqr$model,  
                                   "No penalty" = "both_no_lime_imputation_from_3_5_classes", 
                                   "Squared penalty, lambda 1" = "both_no_3_5_classes_penatly_1_squared",  
                                   "Squared penalty, lambda 0.5" = "both_no_3_5_classes_penatly_05_squared", 
                                   "Squared penalty, lambda 0.1" = "both_no_3_5_classes_penatly_01_squared", 
                                   "Squared penalty, lambda 0.05" = "both_no_3_5_classes_penatly_005_squared", 
                                   "Squared penalty, lambda 0.01" = "both_no_3_5_classes_penatly_001_squared", 
                                   "Squared penalty, lambda 0.005" = "both_no_3_5_classes_penatly_0005_squared", 
                                   "Squared penalty, lambda 0.001" = "both_no_3_5_classes_penatly_0001_squared")

lambda_df_sqr$model <- ordered(lambda_df_sqr$model, levels =c("No penalty","Squared penalty, lambda 1", "Squared penalty, lambda 0.5", "Squared penalty, lambda 0.1", "Squared penalty, lambda 0.05","Squared penalty, lambda 0.01", "Squared penalty, lambda 0.005", "Squared penalty, lambda 0.001"))


analyze_output(lambda_df_sqr, outcome = "pH")
analyze_output(lambda_df_sqr, outcome = "lime")
```
Best prediction: 0.01, actually better than no penalty. And 100% consistent. 


```{r plots_for_report, fig.width = 8, fig.height=8}
xtable(metrics_by_model(lambda_df_sqr, "lime"), digits = 3)
```



```{r}
# Transform the models into the format for consistency evaluation 
# Create base dataframe with observed values to join with
df_conistency_sqr <- read.csv(file= "../data/prepared_data/y_valid.csv")|>
  dplyr::select(pH, lime) |>
  mutate(pH = (pH * ph_std) + ph_mean,
         lime = (lime * lime_std) + lime_mean,
         model = "Observed values")
# add index to later join by
df_conistency_sqr$id <- seq.int(nrow(df_conistency_sqr))

df_conistency_sqr <- extract_best_model("lambda_parameter_tuning_squared", best_model_name = "both_no_lime_imputation_from_3_5_classes", rename_to = "No penalty", df_conistency_sqr)
df_conistency_sqr <- extract_best_model("lambda_parameter_tuning_squared", best_model_name = "both_no_3_5_classes_penatly_1_squared", rename_to = "Squared penalty, lambda 1", df_conistency_sqr)
df_conistency_sqr <- extract_best_model("lambda_parameter_tuning_squared", best_model_name = "both_no_3_5_classes_penatly_05_squared", rename_to = "Squared penalty, lambda 0.5", df_conistency_sqr)
df_conistency_sqr <- extract_best_model("lambda_parameter_tuning_squared", best_model_name = "both_no_3_5_classes_penatly_01_squared", rename_to = "Squared penalty, lambda 0.1",df_conistency_sqr)
df_conistency_sqr <- extract_best_model("lambda_parameter_tuning_squared", best_model_name = "both_no_3_5_classes_penatly_005_squared", rename_to = "Squared penalty, lambda 0.05",df_conistency_sqr)
df_conistency_sqr <- extract_best_model("lambda_parameter_tuning_squared", best_model_name = "both_no_3_5_classes_penatly_001_squared", rename_to = "Squared penalty, lambda 0.01",df_conistency_sqr)
df_conistency_sqr <- extract_best_model("lambda_parameter_tuning_squared", best_model_name = "both_no_3_5_classes_penatly_0005_squared", rename_to = "Squared penalty, lambda 0.005",df_conistency_sqr)
df_conistency_sqr <- extract_best_model("lambda_parameter_tuning_squared", best_model_name = "both_no_3_5_classes_penatly_0001_squared", rename_to = "Squared penalty, lambda 0.001",df_conistency_sqr)
df_conistency_sqr

# Confusion matrix
make_ph_lime_confusion_matrix(df_conistency_sqr, model_name = "No penalty")  
make_ph_lime_confusion_matrix(df_conistency_sqr, model_name = "Squared penalty, lambda 1") 
make_ph_lime_confusion_matrix(df_conistency_sqr, model_name = "Squared penalty, lambda 0.5") 
make_ph_lime_confusion_matrix(df_conistency_sqr, model_name = "Squared penalty, lambda 0.1")
make_ph_lime_confusion_matrix(df_conistency_sqr, model_name = "Squared penalty, lambda 0.05")
make_ph_lime_confusion_matrix(df_conistency_sqr, model_name = "Squared penalty, lambda 0.01")
make_ph_lime_confusion_matrix(df_conistency_sqr, model_name = "Squared penalty, lambda 0.005")
make_ph_lime_confusion_matrix(df_conistency_sqr, model_name = "Squared penalty, lambda 0.001")

# Metrics
metrics_of_rule_points(df_conistency_sqr)

# Histograms
histogram_lime_below_65(df_conistency_sqr) 

# Scatterplot of pH against lime
predicted_against_predicted(df_conistency_sqr) 
```


# Evaluation of best models
## Accuracy
```{r}
best_models <- read.csv(file="./tuning_results/test_predictions_best_models.csv")|>
  pivot_to_pred_model()

best_models$model <-  fct_recode(best_models$model,  "Univariate pH" = "pH_no_lime_imputation_from_3_5_classes", "Univariate Lime" = "lime_no_lime_imputation_from_3_5_classes",  "Multivariate" = "both_no_lime_imputation_from_3_5_classes", "Informed Multivariate" = "both_no_3_5_classes_penatly_001_squared")

best_models$model<- ordered(best_models$model, levels =c("Univariate pH", "Univariate Lime", "Multivariate", "Informed Multivariate"))

best_models
```

```{r}
metrics_by_model(best_models, outcome = "pH")
metrics_by_model(best_models, outcome = "lime")
```

```{r plots_for_poster_ 1, fig.width = 8, fig.height=6}

a <- plot_observed_predicted(best_models, outcome = "pH")
ggsave(a , file = "../Figures/scatter_pH_best_test.pdf")
b <- plot_observed_predicted(best_models, outcome = "lime")

ggarrange(
  a, b, labels = c("A", "B"),
  ncol = 1, nrow = 2
  )

ggsave(file = "../Figures/scatter_pred_observed_best_test.pdf")
```

```{r residuals_best_models, fig.width = 8, fig.height=6}
a <- plot_residuals(best_models, outcome = "pH")
ggsave(a , file = "../Figures/scatter_pH_best_test.pdf")
b <- plot_residuals(best_models, outcome = "lime")

ggarrange(
  a, b, labels = c("A", "B"),
  ncol = 1, nrow = 2
  )

ggsave(file = "../Figures/residuals_best_test.pdf")

```

```{r compare_output_distributions}
a <- plot_compare_output_distributions(best_models, outcome = "pH")
b <- plot_compare_output_distributions(best_models, outcome = "lime")

ggarrange(
  a, b, labels = c("A", "B"),
  ncol = 1, nrow = 2
  )

ggsave(file = "../Figures/pred_vs_observed_dist_best_test.pdf")
```

## Consitency
```{r}
# Prepare parameters for de-scaling pH and lime content
scaling_parameters <- read.csv(file= "scaling_parameters.csv")

ph_mean <- scaling_parameters$mean[scaling_parameters$variable=="pH"]
ph_std <- scaling_parameters$sd[scaling_parameters$variable=="pH"]
lime_mean <- scaling_parameters$mean[scaling_parameters$variable=="lime"]
lime_std <- scaling_parameters$sd[scaling_parameters$variable=="lime"]

# Create base dataframe with observed values to join with
df_best_models <- read.csv(file= "../data/prepared_data/y_test.csv")|>
  dplyr::select(pH, lime) |>
  mutate(pH = (pH * ph_std) + ph_mean,
         lime = (lime * lime_std) + lime_mean,
         model = "Observed values")
# add index to later join by
df_best_models$id <- seq.int(nrow(df_best_models))

df_best_models <- extract_best_model_univariate("test_predictions_best_models", best_model_name_ph = "pH_no_lime_imputation_from_3_5_classes", best_model_name_lime = "lime_no_lime_imputation_from_3_5_classes", rename_to = "Univariate", df_best_models)

df_best_models <- extract_best_model("test_predictions_best_models", best_model_name = "both_no_lime_imputation_from_3_5_classes", rename_to = "Multivariate",df_best_models)

df_best_models <- extract_best_model("test_predictions_best_models", best_model_name = "both_no_3_5_classes_penatly_001_squared", rename_to = "Informed Multivariate",df_best_models)

df_best_models$model<- factor(df_best_models$model, levels =c("Observed values", "Univariate", "Multivariate", "Informed Multivariate"))

```

```{r measures_consit_best_plot}
metrics_of_rule_points(df_best_models)
```

```{r plots_consit_best_models, fig.width = 7, fig.height=4}
histogram_lime_below_65(df_best_models)
ggsave(file = "../Figures/hist_below_65_best_test.pdf")
predicted_against_predicted(df_best_models)
ggsave(file = "../Figures/relationship_pH_lime_best_test.pdf")
```

```{r conf_matrix_best_models}
make_ph_lime_confusion_matrix(df_best_models, "Observed values" )
make_ph_lime_confusion_matrix(df_best_models, "Univariate" )
make_ph_lime_confusion_matrix(df_best_models, "Multivariate" )
make_ph_lime_confusion_matrix(df_best_models, "Informed Multivariate" )
```





## Checking impact of depth

```{r}
x_test <- read.csv(file='../data/prepared_data/x_test.csv')
y_test <- read.csv(file="./tuning_results/test_predictions_best_models.csv")
combined <- cbind(x_test, y_test)
combined
```
```{r}
ggplot(data = combined, aes(x = depth, y = pH_no_lime_imputation_from_3_5_classes.pH))+
  geom_point()

ggplot(data = combined, aes(x = depth, y = lime_no_lime_imputation_from_3_5_classes.lime))+
  geom_point()

ggplot(data = combined, aes(x = depth, y = both_no_3_5_classes_penatly_001_squared.pH))+
  geom_point()

ggplot(data = combined, aes(x = depth, y = both_no_lime_imputation_from_3_5_classes.lime))+
  geom_point()

ggplot(data = combined, aes(x = depth, y = both_no_3_5_classes_penatly_001_squared.pH))+
  geom_point()

ggplot(data = combined, aes(x = depth, y = both_no_3_5_classes_penatly_001_squared.lime))+
  geom_point()
```



# Extract tables into latex 

```{r test_latex, results=tex}

xtable(metrics_by_model())

```

```{r}
#xtable(metrics_by_model(df_imp_both, outcome = "pH"), digits = 3)
xtable(metrics_by_model(df_imp_both, outcome = "lime"), digits = 3)
```